{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9b0d1559-ff3f-4907-a1f4-a760c1234184",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "9b0d1559-ff3f-4907-a1f4-a760c1234184",
        "outputId": "bd3f6e97-fa4b-48ff-f3a4-1fb9903ca914"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/premier-league-matches.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-678692997.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/premier-league-matches.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/premier-league-matches.csv'"
          ]
        }
      ],
      "source": [
        "#This code imports the pandas and matplotlib libraries, reads a CSV file containing Premier League match data into a DataFrame named df.\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = 'data/premier-league-matches.csv'\n",
        "\n",
        "df = pd.read_csv(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f3f6c72-f6c6-4eca-8324-f6cb665ef959",
      "metadata": {
        "id": "0f3f6c72-f6c6-4eca-8324-f6cb665ef959"
      },
      "outputs": [],
      "source": [
        "#This code processes match outcomes and calculates goal differences.\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "df['HomeWin'] = (df['FTR'] == 'H').astype(int)\n",
        "df['AwayWin'] = (df['FTR'] == 'A').astype(int)\n",
        "df['Draw'] = (df['FTR'] == 'D').astype(int)\n",
        "\n",
        "df['GoalDifference'] = df['HomeGoals'] - df['AwayGoals']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2749366-f24a-4b8b-ac1a-c1c9323243a8",
      "metadata": {
        "id": "e2749366-f24a-4b8b-ac1a-c1c9323243a8"
      },
      "outputs": [],
      "source": [
        "#This code calculates team statistics, points, and rankings for each season based on match outcomes and goal data.\n",
        "\n",
        "for season in df['Season_End_Year'].unique():\n",
        "    season_data = df[df['Season_End_Year'] == season]\n",
        "\n",
        "    home_stats = season_data.groupby('Home').agg(\n",
        "        Wins=('FTR', lambda x: (x == 'H').sum()),\n",
        "        Draws=('FTR', lambda x: (x == 'D').sum()),\n",
        "        Losses=('FTR', lambda x: (x == 'A').sum()),\n",
        "        Goals_Scored=('HomeGoals', 'sum'),\n",
        "        Goals_Conceded=('AwayGoals', 'sum')\n",
        "    ).reset_index().rename(columns={'Home': 'Team'})\n",
        "\n",
        "    away_stats = season_data.groupby('Away').agg(\n",
        "        Wins=('FTR', lambda x: (x == 'A').sum()),\n",
        "        Draws=('FTR', lambda x: (x == 'D').sum()),\n",
        "        Losses=('FTR', lambda x: (x == 'H').sum()),\n",
        "        Goals_Scored=('AwayGoals', 'sum'),\n",
        "        Goals_Conceded=('HomeGoals', 'sum')\n",
        "    ).reset_index().rename(columns={'Away': 'Team'})\n",
        "\n",
        "    combined_stats = pd.concat([home_stats, away_stats]).groupby('Team').sum().reset_index()\n",
        "\n",
        "    combined_stats['Goal_Difference'] = combined_stats['Goals_Scored'] - combined_stats['Goals_Conceded']\n",
        "    combined_stats['Points'] = combined_stats['Wins'] * 3 + combined_stats['Draws']\n",
        "\n",
        "    ranking = combined_stats.sort_values(by=['Points', 'Goal_Difference', 'Goals_Scored'], ascending=False)\n",
        "    ranking['Rank'] = range(1, len(ranking) + 1)\n",
        "    ranking['Season_End_Year'] = season"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77ca5d32-72e7-429d-bbf8-cf4c46a6aa90",
      "metadata": {
        "id": "77ca5d32-72e7-429d-bbf8-cf4c46a6aa90"
      },
      "outputs": [],
      "source": [
        "#This code calculates team rankings and adds previous season ranks to the dataset.\n",
        "\n",
        "rankings_dict = {}\n",
        "\n",
        "for season in df['Season_End_Year'].unique():\n",
        "    season_data = df[df['Season_End_Year'] == season]\n",
        "\n",
        "    home_stats = season_data.groupby('Home').agg(\n",
        "        Wins=('FTR', lambda x: (x == 'H').sum()),\n",
        "        Draws=('FTR', lambda x: (x == 'D').sum()),\n",
        "        Losses=('FTR', lambda x: (x == 'A').sum()),\n",
        "        Goals_Scored=('HomeGoals', 'sum'),\n",
        "        Goals_Conceded=('AwayGoals', 'sum')\n",
        "    ).reset_index().rename(columns={'Home': 'Team'})\n",
        "\n",
        "    away_stats = season_data.groupby('Away').agg(\n",
        "        Wins=('FTR', lambda x: (x == 'A').sum()),\n",
        "        Draws=('FTR', lambda x: (x == 'D').sum()),\n",
        "        Losses=('FTR', lambda x: (x == 'H').sum()),\n",
        "        Goals_Scored=('AwayGoals', 'sum'),\n",
        "        Goals_Conceded=('HomeGoals', 'sum')\n",
        "    ).reset_index().rename(columns={'Away': 'Team'})\n",
        "\n",
        "    combined_stats = pd.concat([home_stats, away_stats]).groupby('Team').sum().reset_index()\n",
        "    combined_stats['Goal_Difference'] = combined_stats['Goals_Scored'] - combined_stats['Goals_Conceded']\n",
        "    combined_stats['Points'] = combined_stats['Wins'] * 3 + combined_stats['Draws']\n",
        "    ranking = combined_stats.sort_values(by=['Points', 'Goal_Difference', 'Goals_Scored'], ascending=False)\n",
        "    ranking['Rank'] = range(1, len(ranking) + 1)\n",
        "\n",
        "    previous_season_teams = set(rankings_dict[season - 1].keys()) if (season - 1) in rankings_dict else set()\n",
        "    current_season_teams = set(ranking['Team'])\n",
        "    promoted_teams = current_season_teams - previous_season_teams\n",
        "\n",
        "    lowest_rank = ranking['Rank'].max() + 1\n",
        "    for team in promoted_teams:\n",
        "        ranking.loc[ranking['Team'] == team, 'Rank'] = lowest_rank\n",
        "\n",
        "    rankings_dict[season] = ranking.set_index('Team')['Rank'].to_dict()\n",
        "\n",
        "def get_last_season_rank(team, season):\n",
        "    if season - 1 not in rankings_dict:\n",
        "        return None\n",
        "    return rankings_dict[season - 1].get(team, max(rankings_dict[season - 1].values()) + 1)\n",
        "\n",
        "df['Home_Last_Season_Rank'] = df.apply(lambda row: get_last_season_rank(row['Home'], row['Season_End_Year']), axis=1)\n",
        "df['Away_Last_Season_Rank'] = df.apply(lambda row: get_last_season_rank(row['Away'], row['Season_End_Year']), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41c5eb1d-7033-4ec1-a9f4-c45ac38d087f",
      "metadata": {
        "id": "41c5eb1d-7033-4ec1-a9f4-c45ac38d087f"
      },
      "outputs": [],
      "source": [
        "#This code calculates average goals scored and conceded per team per season.\n",
        "\n",
        "team_season_stats = {}\n",
        "\n",
        "for season in df['Season_End_Year'].unique():\n",
        "    season_data = df[df['Season_End_Year'] == season]\n",
        "\n",
        "    home_stats = season_data.groupby('Home').agg(\n",
        "        Goals_Scored=('HomeGoals', 'sum'),\n",
        "        Goals_Conceded=('AwayGoals', 'sum'),\n",
        "        Matches=('HomeGoals', 'count')\n",
        "    ).reset_index().rename(columns={'Home': 'Team'})\n",
        "\n",
        "    away_stats = season_data.groupby('Away').agg(\n",
        "        Goals_Scored=('AwayGoals', 'sum'),\n",
        "        Goals_Conceded=('HomeGoals', 'sum'),\n",
        "        Matches=('AwayGoals', 'count')\n",
        "    ).reset_index().rename(columns={'Away': 'Team'})\n",
        "\n",
        "    combined_stats = pd.concat([home_stats, away_stats])\n",
        "    combined_stats = combined_stats.groupby('Team').sum().reset_index()\n",
        "\n",
        "    combined_stats['Avg_Goals_Scored'] = combined_stats['Goals_Scored'] / combined_stats['Matches']\n",
        "    combined_stats['Avg_Goals_Conceded'] = combined_stats['Goals_Conceded'] / combined_stats['Matches']\n",
        "\n",
        "    team_season_stats[season] = combined_stats.set_index('Team')[['Avg_Goals_Scored', 'Avg_Goals_Conceded']].to_dict('index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f064cc8-45f8-4fd2-9130-f633933b6f81",
      "metadata": {
        "id": "0f064cc8-45f8-4fd2-9130-f633933b6f81"
      },
      "outputs": [],
      "source": [
        "#This function retrieves a team's average stat from the previous season.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def get_prev_season_avg(team, season, stat):\n",
        "    prev_season = season - 1\n",
        "    if prev_season in team_season_stats:\n",
        "        return team_season_stats[prev_season].get(team, {}).get(stat, np.nan)\n",
        "    else:\n",
        "        return np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55873c6b-7c72-46ee-964b-abf8496cb81a",
      "metadata": {
        "id": "55873c6b-7c72-46ee-964b-abf8496cb81a"
      },
      "outputs": [],
      "source": [
        "#This code adds columns for each team's average goals scored and conceded in the previous season for both home and away teams.\n",
        "\n",
        "df['Home_Avg_Goals_Scored_Prev_Season'] = df.apply(\n",
        "    lambda row: get_prev_season_avg(row['Home'], row['Season_End_Year'], 'Avg_Goals_Scored'), axis=1)\n",
        "df['Home_Avg_Goals_Conceded_Prev_Season'] = df.apply(\n",
        "    lambda row: get_prev_season_avg(row['Home'], row['Season_End_Year'], 'Avg_Goals_Conceded'), axis=1)\n",
        "\n",
        "df['Away_Avg_Goals_Scored_Prev_Season'] = df.apply(\n",
        "    lambda row: get_prev_season_avg(row['Away'], row['Season_End_Year'], 'Avg_Goals_Scored'), axis=1)\n",
        "df['Away_Avg_Goals_Conceded_Prev_Season'] = df.apply(\n",
        "    lambda row: get_prev_season_avg(row['Away'], row['Season_End_Year'], 'Avg_Goals_Conceded'), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c31836ee-dacc-492b-8f6c-8564afa3cafe",
      "metadata": {
        "id": "c31836ee-dacc-492b-8f6c-8564afa3cafe"
      },
      "outputs": [],
      "source": [
        "#This code calculates league-wide average goals scored and conceded for each season.\n",
        "\n",
        "league_avg = {}\n",
        "\n",
        "for season in team_season_stats.keys():\n",
        "    season_teams = team_season_stats[season]\n",
        "    avg_scored = np.mean([stats['Avg_Goals_Scored'] for stats in season_teams.values()])\n",
        "    avg_conceded = np.mean([stats['Avg_Goals_Conceded'] for stats in season_teams.values()])\n",
        "    league_avg[season] = {\n",
        "        'Avg_Goals_Scored': avg_scored,\n",
        "        'Avg_Goals_Conceded': avg_conceded\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f07a0b71-19dc-4296-befa-1f8e68efcf48",
      "metadata": {
        "id": "f07a0b71-19dc-4296-befa-1f8e68efcf48"
      },
      "outputs": [],
      "source": [
        "#This code fills missing values in average goals columns with league-wide averages from the previous season.\n",
        "\n",
        "columns_to_fill = [\n",
        "    ('Home_Avg_Goals_Scored_Prev_Season', 'Avg_Goals_Scored'),\n",
        "    ('Home_Avg_Goals_Conceded_Prev_Season', 'Avg_Goals_Conceded'),\n",
        "    ('Away_Avg_Goals_Scored_Prev_Season', 'Avg_Goals_Scored'),\n",
        "    ('Away_Avg_Goals_Conceded_Prev_Season', 'Avg_Goals_Conceded')\n",
        "]\n",
        "\n",
        "for col_name, stat_name in columns_to_fill:\n",
        "    df[col_name] = df.apply(\n",
        "        lambda row: league_avg.get(row['Season_End_Year'] - 1, {}).get(stat_name)\n",
        "        if pd.isna(row[col_name]) else row[col_name], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caa7c909-7492-4ad4-9d39-6cb6cf4649a5",
      "metadata": {
        "id": "caa7c909-7492-4ad4-9d39-6cb6cf4649a5"
      },
      "outputs": [],
      "source": [
        "#This code calculates cumulative points for home and away teams throughout each season, updating match-by-match based on chronological order.\n",
        "\n",
        "df['HomePoints'] = df['HomeWin'] * 3 + df['Draw'] * 1\n",
        "df['AwayPoints'] = df['AwayWin'] * 3 + df['Draw'] * 1\n",
        "\n",
        "df['Home_CumulativePoints'] = 0\n",
        "df['Away_CumulativePoints'] = 0\n",
        "\n",
        "for season in df['Season_End_Year'].unique():\n",
        "    season_data = df[df['Season_End_Year'] == season]\n",
        "\n",
        "    cumulative_points = {team: 0 for team in pd.concat([season_data['Home'], season_data['Away']]).unique()}\n",
        "\n",
        "    for idx, row in season_data.sort_values(by='Date').iterrows():\n",
        "        home_team = row['Home']\n",
        "        away_team = row['Away']\n",
        "\n",
        "        df.loc[idx, 'Home_CumulativePoints'] = cumulative_points[home_team]\n",
        "        df.loc[idx, 'Away_CumulativePoints'] = cumulative_points[away_team]\n",
        "\n",
        "        cumulative_points[home_team] += row['HomePoints']\n",
        "        cumulative_points[away_team] += row['AwayPoints']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1569daf1-1a7a-4894-958f-7adcabf9711f",
      "metadata": {
        "id": "1569daf1-1a7a-4894-958f-7adcabf9711f"
      },
      "outputs": [],
      "source": [
        "required_columns = {'Date', 'Home', 'Away', 'FTR'}\n",
        "missing_cols = required_columns - set(df.columns)\n",
        "if missing_cols:\n",
        "    raise ValueError(f\"Input DataFrame is missing required columns: {missing_cols}\")\n",
        "\n",
        "if not pd.api.types.is_datetime64_any_dtype(df['Date']):\n",
        "    raise TypeError(\"The 'Date' column must be of a datetime type.\")\n",
        "\n",
        "df = df.sort_values(by='Date')\n",
        "\n",
        "def assign_form_score(row):\n",
        "    if row['FTR'] == 'H':\n",
        "        return 3\n",
        "    elif row['FTR'] == 'A':\n",
        "        return 3\n",
        "    elif row['FTR'] == 'D':\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "df['Match_Score'] = df.apply(assign_form_score, axis=1)\n",
        "\n",
        "def calculate_rolling_average_for_team(team_df, window=5):\n",
        "    team_df = team_df.sort_values(by='Date')\n",
        "\n",
        "    home_wins = (team_df['FTR'] == 'H').astype(int) * 3\n",
        "    home_draws = (team_df['FTR'] == 'D').astype(int) * 1\n",
        "    home_points = home_wins + home_draws\n",
        "\n",
        "    away_wins = (team_df['FTR'] == 'A').astype(int) * 3\n",
        "    away_draws = (team_df['FTR'] == 'D').astype(int) * 1\n",
        "    away_points = away_wins + away_draws\n",
        "\n",
        "    team_df['Team_Point'] = 0\n",
        "    team_df.loc[team_df['Home'] == team_df['Team'], 'Team_Point'] = home_points\n",
        "    team_df.loc[team_df['Away'] == team_df['Team'], 'Team_Point'] = away_points\n",
        "\n",
        "    team_df['Team_Rolling_Avg'] = (\n",
        "        team_df['Team_Point']\n",
        "        .rolling(window=window, min_periods=1)\n",
        "        .mean()\n",
        "        .shift(1, fill_value=0)\n",
        "    )\n",
        "\n",
        "    return team_df\n",
        "\n",
        "all_teams = pd.unique(df[['Home', 'Away']].values.ravel())\n",
        "rolling_df = pd.DataFrame()\n",
        "\n",
        "for team in all_teams:\n",
        "    team_matches = df[(df['Home'] == team) | (df['Away'] == team)].copy()\n",
        "    team_matches['Team'] = team\n",
        "    team_rolled = calculate_rolling_average_for_team(team_matches)\n",
        "    rolling_df = pd.concat([rolling_df, team_rolled], ignore_index=True)\n",
        "\n",
        "df = df.merge(\n",
        "    rolling_df[['Date', 'Team', 'Team_Rolling_Avg']],\n",
        "    left_on=['Date', 'Home'],\n",
        "    right_on=['Date', 'Team'],\n",
        "    how='left'\n",
        ")\n",
        "df.rename(columns={'Team_Rolling_Avg': 'Home_Rolling_Avg'}, inplace=True)\n",
        "df.drop(columns=['Team'], inplace=True)\n",
        "\n",
        "df = df.merge(\n",
        "    rolling_df[['Date', 'Team', 'Team_Rolling_Avg']],\n",
        "    left_on=['Date', 'Away'],\n",
        "    right_on=['Date', 'Team'],\n",
        "    how='left'\n",
        ")\n",
        "df.rename(columns={'Team_Rolling_Avg': 'Away_Rolling_Avg'}, inplace=True)\n",
        "df.drop(columns=['Team'], inplace=True)\n",
        "\n",
        "print(df[['Date', 'Home', 'Away', 'Home_Rolling_Avg', 'Away_Rolling_Avg']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23229bda-1de5-4e06-99c7-36e3e00a9a5d",
      "metadata": {
        "id": "23229bda-1de5-4e06-99c7-36e3e00a9a5d"
      },
      "outputs": [],
      "source": [
        "teams_2023 = df[df['Season_End_Year'] == 2023]\n",
        "\n",
        "unique_teams_2023 = pd.unique(pd.concat([teams_2023['Home'], teams_2023['Away']]))\n",
        "\n",
        "print(unique_teams_2023)\n",
        "\n",
        "unique_values = df['Home'].unique()\n",
        "print(unique_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ad4b82b-4731-481d-a8b5-e42e537296f5",
      "metadata": {
        "id": "3ad4b82b-4731-481d-a8b5-e42e537296f5"
      },
      "outputs": [],
      "source": [
        "#Data shaping for training\n",
        "teams = ['Coventry City', 'Southampton', 'Everton', 'Ipswich Town',\n",
        "         'Chelsea', 'Crystal Palace', 'Sheffield Utd', 'Leeds United',\n",
        "         'Arsenal', \"Nott'ham Forest\", 'Manchester City', 'Blackburn',\n",
        "         'Wimbledon', 'QPR', 'Sheffield Weds', 'Manchester Utd', 'Norwich City',\n",
        "         'Tottenham', 'Oldham Athletic', 'Aston Villa', 'Liverpool', 'Middlesbrough',\n",
        "         'West Ham', 'Newcastle Utd', 'Swindon Town', 'Leicester City', 'Bolton',\n",
        "         'Derby County', 'Sunderland', 'Barnsley', 'Charlton Ath', 'Watford',\n",
        "         'Bradford City', 'Fulham', 'West Brom', 'Birmingham City', 'Portsmouth',\n",
        "         'Wolves', 'Wigan Athletic', 'Reading', 'Hull City', 'Stoke City',\n",
        "         'Burnley', 'Blackpool', 'Swansea City', 'Cardiff City',\n",
        "         'Bournemouth', 'Brighton', 'Huddersfield', 'Brentford'\n",
        "]\n",
        "\n",
        "team_dfs = []\n",
        "\n",
        "for team in teams:\n",
        "    team_df = df[(df['Home'] == team) | (df['Away'] == team)].copy()\n",
        "\n",
        "    team_df['Target_Team'] = team\n",
        "\n",
        "    team_df['MatchWin'] = np.where(\n",
        "        ((team_df['Home'] == team) & (team_df['FTR'] == 'H')) |\n",
        "        ((team_df['Away'] == team) & (team_df['FTR'] == 'A')),\n",
        "        1, 0\n",
        "    )\n",
        "\n",
        "    team_dfs.append(team_df)\n",
        "\n",
        "combined_df = pd.concat(team_dfs, ignore_index=True)\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoders = {col: LabelEncoder() for col in ['Home', 'Away', 'Target_Team']}\n",
        "\n",
        "for col, encoder in label_encoders.items():\n",
        "    combined_df[col] = encoder.fit_transform(combined_df[col])\n",
        "\n",
        "features = [\n",
        "    'Home', 'Away', 'Target_Team',\n",
        "    'Home_Last_Season_Rank', 'Away_Last_Season_Rank',\n",
        "    'Home_Avg_Goals_Scored_Prev_Season', 'Home_Avg_Goals_Conceded_Prev_Season',\n",
        "    'Away_Avg_Goals_Scored_Prev_Season', 'Away_Avg_Goals_Conceded_Prev_Season',\n",
        "    'Home_CumulativePoints', 'Away_CumulativePoints',\n",
        "    'Home_Rolling_Avg', 'Away_Rolling_Avg',\n",
        "]\n",
        "target = 'MatchWin'\n",
        "\n",
        "filtered_data = combined_df[combined_df['Season_End_Year'] > 1993]\n",
        "\n",
        "train_data = filtered_data[filtered_data['Season_End_Year'] < 2023]\n",
        "test_data = filtered_data[filtered_data['Season_End_Year'] == 2023]\n",
        "\n",
        "X_train = train_data[features].values\n",
        "y_train = train_data[target].values\n",
        "X_test = test_data[features].values\n",
        "y_test = test_data[target].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8184bcc6-ad21-4049-87b0-8b9e10454485",
      "metadata": {
        "id": "8184bcc6-ad21-4049-87b0-8b9e10454485"
      },
      "outputs": [],
      "source": [
        "print(\"Training Data Shape:\")\n",
        "print(f\"Features (X_train): {X_train.shape}\")\n",
        "print(f\"Labels (y_train): {y_train.shape}\")\n",
        "\n",
        "print(\"Test Data Shape:\")\n",
        "print(f\"Features (X_test): {X_test.shape}\")\n",
        "print(f\"Labels (y_test): {y_test.shape}\")\n",
        "\n",
        "print(f\"Number of rows in combined_df: {combined_df.shape[0]}\")\n",
        "print(f\"Number of rows in train_data: {train_data.shape[0]}\")\n",
        "print(f\"Number of rows in test_data: {test_data.shape[0]}\")\n",
        "\n",
        "print(\"Columns in combined_df:\")\n",
        "print(combined_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "643b3ee4-693e-479f-baac-78156a63fd7f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "643b3ee4-693e-479f-baac-78156a63fd7f",
        "outputId": "2adeee7a-e994-4430-f0d5-3aeb8504e02a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value counts of MatchWin:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'combined_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2944214848.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Value counts of MatchWin:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MatchWin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nPercentage distribution of MatchWin:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MatchWin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'combined_df' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"Value counts of MatchWin:\")\n",
        "print(combined_df['MatchWin'].value_counts())\n",
        "\n",
        "print(\"\\nPercentage distribution of MatchWin:\")\n",
        "print(combined_df['MatchWin'].value_counts(normalize=True) * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3384f254-38ad-4361-98e8-dd6bc308ad43",
      "metadata": {
        "id": "3384f254-38ad-4361-98e8-dd6bc308ad43"
      },
      "outputs": [],
      "source": [
        "#This code calculates and visualizes a Pearson correlation matrix for selected columns in Liverpool matches using a heatmap.\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def perform_correlation_analysis(combined_df, selected_columns):\n",
        "    for col in selected_columns:\n",
        "        if col not in combined_df.columns:\n",
        "            raise ValueError(f\"Column '{col}' is not in the DataFrame.\")\n",
        "\n",
        "    numeric_df = combined_df[selected_columns].select_dtypes(include=['number'])\n",
        "    correlation_matrix = numeric_df.corr(method='pearson')\n",
        "    return correlation_matrix\n",
        "\n",
        "selected_columns = ['HomeGoals', 'AwayGoals', 'HomeWin', 'AwayWin', 'Draw', 'GoalDifference', 'Home_Last_Season_Rank', 'Away_Last_Season_Rank', 'Home_Avg_Goals_Scored_Prev_Season', 'Home_Avg_Goals_Conceded_Prev_Season', 'Away_Avg_Goals_Scored_Prev_Season', 'Away_Avg_Goals_Conceded_Prev_Season','Home_CumulativePoints', 'Away_CumulativePoints', 'Home_Rolling_Avg', 'Away_Rolling_Avg']\n",
        "correlation_matrix = perform_correlation_analysis(combined_df, selected_columns)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Matrix for Numeric Features')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cdc0dd5-c514-4e37-81a2-556ed1ddd2e9",
      "metadata": {
        "id": "4cdc0dd5-c514-4e37-81a2-556ed1ddd2e9"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "print(\"Running GridSearch for Random Forest with params:\", rf_param_grid)\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_grid_search = GridSearchCV(\n",
        "    estimator=rf_model,\n",
        "    param_grid=rf_param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_rf_model = rf_grid_search.best_estimator_\n",
        "\n",
        "rf_results_df = pd.DataFrame(rf_grid_search.cv_results_)\n",
        "print(\"\\n=== Random Forest Grid Search Results (Top 5) ===\")\n",
        "print(rf_results_df[['params', 'mean_test_score', 'rank_test_score']].sort_values('rank_test_score').head(5))\n",
        "\n",
        "print(\"\\n=== Best Random Forest Params ===\")\n",
        "print(rf_grid_search.best_params_)\n",
        "print(\"Best RF Score:\", rf_grid_search.best_score_)\n",
        "\n",
        "xgb_param_grid = {\n",
        "    'learning_rate': [0.1, 0.05],\n",
        "    'max_depth': [3, 6],\n",
        "    'n_estimators': [100, 200, 300]\n",
        "}\n",
        "\n",
        "print(\"\\nRunning GridSearch for XGBoost with params:\", xgb_param_grid)\n",
        "xgb_model = XGBClassifier(\n",
        "    eval_metric='logloss',\n",
        "    random_state=42\n",
        ")\n",
        "xgb_grid_search = GridSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_grid=xgb_param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "xgb_grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_xgb_model = xgb_grid_search.best_estimator_\n",
        "\n",
        "xgb_results_df = pd.DataFrame(xgb_grid_search.cv_results_)\n",
        "print(\"\\n=== XGBoost Grid Search Results (Top 5) ===\")\n",
        "print(xgb_results_df[['params', 'mean_test_score', 'rank_test_score']].sort_values('rank_test_score').head(5))\n",
        "\n",
        "print(\"\\n=== Best XGBoost Params ===\")\n",
        "print(xgb_grid_search.best_params_)\n",
        "print(\"Best XGB Score:\", xgb_grid_search.best_score_)\n",
        "\n",
        "ensemble_model = VotingClassifier(\n",
        "    estimators=[('rf', best_rf_model), ('xgb', best_xgb_model)],\n",
        "    voting='soft'\n",
        ")\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "models = {\n",
        "    'Random Forest (Optimized)': best_rf_model,\n",
        "    'XGBoost (Optimized)': best_xgb_model,\n",
        "    'Ensemble (RF+XGB)': ensemble_model\n",
        "}\n",
        "\n",
        "results = []\n",
        "for model_name, model in models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "    results.append({\n",
        "        'Model': model_name,\n",
        "        'Accuracy': acc,\n",
        "        'Precision (Class 1)': report['1']['precision'],\n",
        "        'Recall (Class 1)': report['1']['recall'],\n",
        "        'F1-score (Class 1)': report['1']['f1-score']\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(\"\\n=== Final Model Performance Comparison ===\")\n",
        "print(results_df)\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"\\n--- {model_name} ---\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(\n",
        "    results_df['Model'],\n",
        "    results_df['Accuracy'],\n",
        "    color=['skyblue', 'orange', 'green']\n",
        ")\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Model')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc3e5522-5b0a-4ffe-9145-9884f4a1669a",
      "metadata": {
        "id": "bc3e5522-5b0a-4ffe-9145-9884f4a1669a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "be48e0aa-767c-4d74-ac08-93c138150bfd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "be48e0aa-767c-4d74-ac08-93c138150bfd",
        "outputId": "b12aeef2-d1b2-49e2-d8eb-d1ff52039875"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4096554471.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mX_train_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mX_test_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "def create_sequences(X, y, seq_length=5):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(X) - seq_length):\n",
        "        X_seq.append(X[i:i + seq_length])\n",
        "        y_seq.append(y[i + seq_length])\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "sequence_length = 5\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train, sequence_length)\n",
        "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test, sequence_length)\n",
        "\n",
        "hidden_units = 32\n",
        "learning_rate = 0.001\n",
        "epochs = 100\n",
        "batch_size = 16\n",
        "dropout_rate = 0.3\n",
        "\n",
        "input_shape = (sequence_length, X_train_seq.shape[2])\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=input_shape),\n",
        "    SimpleRNN(hidden_units, activation='relu', return_sequences=True),\n",
        "    Dropout(dropout_rate),\n",
        "    BatchNormalization(),\n",
        "    SimpleRNN(hidden_units, activation='relu'),\n",
        "    Dropout(dropout_rate),\n",
        "    BatchNormalization(),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "optimizer = Adam(learning_rate=learning_rate, clipnorm=1.0)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_seq, y_train_seq,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test_seq, y_test_seq, verbose=0)\n",
        "y_pred_probs = model.predict(X_test_seq)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "print(f\"\\nTest Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_seq, y_pred))\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8ad89b8-71cb-443f-9399-f9d96c11dffb",
      "metadata": {
        "id": "c8ad89b8-71cb-443f-9399-f9d96c11dffb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}